{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.colormaps.get('tab10').colors\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from tqdm import trange\n",
    "import os\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import os\n",
    "import torch_geometric as tg\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "class DefectDetectionDataset(tg.data.Dataset):\n",
    "    '''\n",
    "    This class bundles the creation and saving as well as loading of a dataset of 3D graphs. If an instance is created, the class will \n",
    "    check in root directory if the dataset is already processed. If not, the process() method will be called. Furthermore, the\n",
    "    dataset will be loaded. If the dataset shall be calculated again, the process() method must be called explicitely.\n",
    "    '''\n",
    "    def __init__(self, root, n_graphs_per_type=100, transform=None, pre_transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "        - root (str): The directory where the dataset should be stored, divided into processed and raw dirs\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.n_graphs_per_type = n_graphs_per_type\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        '''\n",
    "        If this file exists in the raw directory, the download will be skipped. Download not implemented.\n",
    "        '''\n",
    "        return 'raw.txt'\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        '''\n",
    "        If this file exists in the processed directory, processing will be skipped. \n",
    "        Note: This does smh not work, therefore files are ATM recalculated every time.\n",
    "        '''\n",
    "        return ['data_00000.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        '''\n",
    "        Download not implemented.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def len(self):\n",
    "        '''\n",
    "        Returns the number of graphs in the dataset.\n",
    "        '''\n",
    "        return len([f for f in os.listdir(os.path.join(self.root, 'processed')) if f.startswith('data')])\n",
    "    \n",
    "    def get(self, idx):\n",
    "        '''\n",
    "        Returns the graph at index idx. \n",
    "        '''\n",
    "        data = torch.load(os.path.join(self.processed_dir, 'data_{:05d}.pt'.format(idx)))\n",
    "        return data\n",
    "    \n",
    "    def process(self):\n",
    "        '''\n",
    "        Here creation, processing and saving of the dataset happens. \n",
    "        '''\n",
    "        # Some attributes for all graphs:\n",
    "        self.size = np.array([5,5,5])\n",
    "        lattice_types = {\n",
    "             0: {'name': 'aP', 'nodes': self._get_P_nodes, 'binding_angles': [  0,   0,   0], 'scale': [0, 0, 0]},\n",
    "             1: {'name': 'mP', 'nodes': self._get_P_nodes, 'binding_angles': [ 90,   0,  90], 'scale': [0, 0, 0]},\n",
    "             2: {'name': 'mS', 'nodes': self._get_S_nodes, 'binding_angles': [ 90,   0,  90], 'scale': [0, 0, 0]},\n",
    "             3: {'name': 'oP', 'nodes': self._get_P_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [0, 0, 0]},\n",
    "             4: {'name': 'oS', 'nodes': self._get_S_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [0, 0, 0]},\n",
    "             5: {'name': 'oI', 'nodes': self._get_I_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [0, 0, 0]},\n",
    "             6: {'name': 'oF', 'nodes': self._get_F_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [0, 0, 0]},\n",
    "             7: {'name': 'tP', 'nodes': self._get_P_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [1, 1, 0]},\n",
    "             8: {'name': 'tI', 'nodes': self._get_I_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [1, 1, 0]},\n",
    "             9: {'name': 'hR', 'nodes': self._get_P_nodes, 'binding_angles': [  0,   0,   0], 'scale': [1, 1, 1]},\n",
    "            10: {'name': 'hP', 'nodes': self._get_P_nodes, 'binding_angles': [ 90,  90, 120], 'scale': [1, 1, 0]},\n",
    "            11: {'name': 'cP', 'nodes': self._get_P_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [1, 1, 1]},\n",
    "            12: {'name': 'cI', 'nodes': self._get_I_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [1, 1, 1]},\n",
    "            13: {'name': 'cF', 'nodes': self._get_F_nodes, 'binding_angles': [ 90,  90,  90], 'scale': [1, 1, 1]},\n",
    "        }\n",
    "\n",
    "        \n",
    "        for n in trange(self.n_graphs_per_type * 14):\n",
    "            # Get graph features:\n",
    "            pos, edge_index, label = self._process_lattice(lattice_types[n % 14])\n",
    "            label = np.expand_dims(label, axis=1)\n",
    "            node_attr = self._get_node_attr(pos, edge_index)\n",
    "            edge_attr = self._get_edge_attr(pos, edge_index)\n",
    "            # Create data object:\n",
    "            data = tg.data.Data(x          = torch.tensor(node_attr, dtype=torch.float), \n",
    "                                edge_index = torch.tensor(edge_index, dtype=torch.int64), \n",
    "                                edge_attr  = torch.tensor(edge_attr, dtype=torch.float), \n",
    "                                y          = torch.tensor(label, dtype=torch.float), \n",
    "                                pos        = torch.tensor(pos, dtype=torch.float))\n",
    "            # Save data object:\n",
    "            torch.save(data, os.path.join(self.processed_dir, 'data_{:05d}.pt'.format(n)))\n",
    "\n",
    "\n",
    "    def _get_P_nodes(self, angles=np.array([90,90,90])):\n",
    "        '''\n",
    "        Get the nodes of a primitive lattice.\n",
    "        '''\n",
    "        scaling = np.sin(np.radians(angles))\n",
    "        vec1 = np.arange(0,self.size[0],1)\n",
    "        vec2 = np.arange(0,self.size[1]*scaling[2],1*scaling[2])\n",
    "        vec3 = np.arange(0,self.size[2]*scaling[1],1*scaling[1])\n",
    "        a, b, c = np.meshgrid(vec1,vec2, vec3)\n",
    "        nodes = np.stack([a,b,c],axis=-1) # Stack them in a new axis\n",
    "        nodes = np.reshape(nodes, (-1, 3)) # Reshape to an arr of nodes with shape (#nodes, 3)\n",
    "        return nodes\n",
    "\n",
    "    def _get_S_nodes(self):\n",
    "        '''\n",
    "        Get the nodes of a base-centred lattice.\n",
    "        '''\n",
    "        P = self._get_P_nodes()\n",
    "        extra = P + np.array([0.5,0.5,0])\n",
    "        return np.append(P,extra,axis=0)\n",
    "\n",
    "    def _get_I_nodes(self):\n",
    "        '''\n",
    "        Get the nodes of a body-centred lattice.\n",
    "        '''\n",
    "        P = self._get_P_nodes()\n",
    "        extra = P + 0.5\n",
    "        return np.append(P,extra,axis=0)\n",
    "\n",
    "    def _get_F_nodes(self):\n",
    "        '''\n",
    "        Get the nodes of a face-centred lattice.\n",
    "        '''\n",
    "        P = self._get_P_nodes()\n",
    "        extra1 = P + np.array([0.5,0.5,0])\n",
    "        extra2 = P + np.array([0,0.5,0.5])\n",
    "        extra3 = P + np.array([0.5,0,0.5])\n",
    "        return np.row_stack((P, extra1, extra2, extra3))\n",
    "    \n",
    "\n",
    "    def _process_lattice(self, arg_dict):\n",
    "        '''\n",
    "        Method that processes a lattice of a given type. The method is called with a dictionary holding parameters for one of the lattice types. It contains the following keys:\n",
    "            - name: The name of the lattice type\n",
    "            - nodes: The method to get the fitting fundamental lattice nodes\n",
    "            - binding_angles: A list of binding angles [alpha, beta, gamma] of the lattice type. Angles are in degrees. 0° means to generate a independent random angle (0,180)°\n",
    "            - scale: A list of scaling factors [x,y,z] for the lattice type. 0 means to generate a random scaling factor (0,2)\n",
    "        '''\n",
    "        # Get lattice angles\n",
    "        angles = np.array(arg_dict['binding_angles'])\n",
    "        if arg_dict['name'] == 'hR':\n",
    "            # Special case for hR lattice as it has 3 identical but random angles\n",
    "            angles = np.where(angles == 0, rng.uniform(46,89,1), angles)\n",
    "        else:\n",
    "            angles = np.where(angles == 0, rng.uniform(46,89,3), angles)\n",
    "            \n",
    "        # Get the fundamental lattice nodes\n",
    "        if arg_dict['name'] in ['hR', 'hP']:\n",
    "            # For hR and hP lattices we need to give the angles to the nodes method so that sheared connections are equally long\n",
    "            nodes = arg_dict['nodes'](angles)\n",
    "        else:\n",
    "            nodes = arg_dict['nodes']()\n",
    "        nodes = self._shear_nodes(nodes, angles)\n",
    "        # Find random scale and apply gaussian noise to the lattice accordingly\n",
    "        scale = np.array(arg_dict['scale'])\n",
    "        scale = np.where(scale == 0, rng.uniform(0.1,3,3), scale)\n",
    "        noise_level = 0.05 / scale  # At this step we scale the noise down, so that the scaling later on does not affect the noise level\n",
    "        nodes += rng.normal(0, noise_level, nodes.shape)\n",
    "        \n",
    "        nodes, labels = self._displace_node(nodes)\n",
    "        # Find the connections between the nodes in a given radius\n",
    "        cons= self._get_cons_in_radius(nodes, 1.3+np.mean(noise_level))\n",
    "        # Apply the saved scaling\n",
    "        nodes *= scale\n",
    "        \n",
    "        # Add defects to the lattice\n",
    "        #nodes, cons, labels = self._add_defects(nodes, cons, labels)\n",
    "        return nodes, cons, labels\n",
    "\n",
    "    def _displace_node(self, nodes):\n",
    "        '''\n",
    "        Method that dislaces one random node in the lattice by a random amount. Returns the new nodes and the label for classification. \n",
    "        The label is a one hot encoded array of shape (len(nodes)) where 1 markes the index od the displaced node.\n",
    "        '''\n",
    "        # Get random node and displacement\n",
    "        node_ind = rng.integers(0, len(nodes))\n",
    "        displacement = rng.normal(0, 1, 3)\n",
    "        # Displace node, get label\n",
    "        nodes[node_ind] += displacement\n",
    "        labels = np.zeros(len(nodes))\n",
    "        labels[node_ind] = 1\n",
    "        return nodes, labels\n",
    "    \n",
    "    def _get_cons_in_radius(self, nodes, radius):\n",
    "        '''\n",
    "        Get the connections in a radius as well as the total number of cons for each node.\n",
    "        '''\n",
    "        tree = KDTree(nodes)\n",
    "        cons = tree.query_pairs(radius, output_type='ndarray', p=2)\n",
    "        cons = cons.T\n",
    "        cons = np.column_stack((cons, cons[::-1])) # Add the reverse connections\n",
    "        return cons\n",
    "\n",
    "    def _shear_nodes(self, nodes, binding_angle):\n",
    "        '''\n",
    "        Shear nodes. Binding angle is a 3D vector with the Binding angle in each axis.\n",
    "        '''\n",
    "        delta = np.tan(np.radians(np.array(binding_angle)))\n",
    "        assert not np.any(delta == 0), 'Binding angle cannot be 0'\n",
    "        nodes = nodes.astype(float)\n",
    "        nodes = nodes + np.stack((nodes[:,2]/delta[0] + nodes[:,1]/delta[2], nodes[:,2]/delta[1] , np.zeros_like(nodes[:,1])), axis=1)\n",
    "        return nodes\n",
    "\n",
    "    def _add_defects(self, nodes, edge_index, labels):\n",
    "        '''\n",
    "        Method that adds up to 10% of random defects (i.e. missing nodes) to the lattice. Should be called after _get_*_graph() but before\n",
    "        _get_edge_attr() and _get_node_attr().\n",
    "        '''\n",
    "        # Draw up to 10% of unique random indices for nodes to be removed\n",
    "        drop_indices = rng.choice(np.arange(len(nodes)), rng.integers(len(nodes)//10), replace=False)\n",
    "        # Remove the nodes and labels\n",
    "        nodes = np.delete(nodes, drop_indices, axis=0)\n",
    "        labels = np.delete(labels, drop_indices, axis=0)\n",
    "        # Delete every connection that refers to a removed node\n",
    "        edge_index = np.delete(edge_index, np.where(np.isin(edge_index, drop_indices))[1], axis=1)\n",
    "        \n",
    "        # As edge_index refers to the original node indices, we need to adjust the indices of most connections\n",
    "        # For this we create a mapping from old indices to new indices\n",
    "        old_to_new = np.arange(len(nodes) + len(drop_indices))  # Start with an array of original indices; [0,1,2,3,4,5,...]\n",
    "        old_to_new[drop_indices] = -1  # Mark the indices of the nodes to be deleted; eg. drop_indices = [1,3] -> [0,-1,2,-1,4,5,...]\n",
    "        old_to_new = np.cumsum(old_to_new != -1) - 1  # Create a cumulative sum array; cumsum([True, False, True, False, True, True,...]) -1 -> [1,1,2,2,3,4,...] -1 -> [0,0,1,1,2,3,...]\n",
    "        \n",
    "        # # Update edge indices to reflect new node indices through broadcasting magic\n",
    "        edge_index = old_to_new[edge_index]\n",
    "        return nodes, edge_index, labels\n",
    "        \n",
    "    def _get_node_attr(self,nodes,cons):\n",
    "        '''\n",
    "        Method that returns the node attributes for each node in the graph. Should be called after creating the graph and adding defects.\n",
    "        Returns an array of shape (len(pos) = #Nodes) with the entries [C] for each node.\n",
    "            - C: Number of connections to other nodes\n",
    "        '''\n",
    "        # Get the number of connections for each node\n",
    "        connection_counts = np.zeros(len(nodes))\n",
    "        for edge in cons[0]:\n",
    "            # Iterate over all edge start points and count the connections for each node. Start points sufficient, as connections are bidirectional.\n",
    "            connection_counts[edge] += 1 \n",
    "            \n",
    "        return np.expand_dims(connection_counts, axis=1)\n",
    "    \n",
    "    def _get_edge_attr(self,nodes,cons):\n",
    "        '''\n",
    "        Method that returns the edge attributes for each edge in the graph. Should be called after creating the graph and adding defects.\n",
    "        Returns an array of shape (len(edge_index[0])= #Edges, 2) with the entries [dx,dy] for each edge.\n",
    "        '''\n",
    "        # Get the edge vectors for each edge\n",
    "        edge_vectors = nodes[cons[0]] - nodes[cons[1]]\n",
    "        return edge_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420/420 [00:02<00:00, 158.52it/s]\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_2060\\2296706662.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.processed_dir, 'data_{:05d}.pt'.format(idx)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[250, 1], edge_index=[2, 3564], edge_attr=[3564, 3], y=[250, 1], pos=[250, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 30\n",
    "dataset = DefectDetectionDataset(root='defect_graphs', n_graphs_per_type=n)\n",
    "dataset.process()\n",
    "train_loader = tg.loader.DataLoader(dataset[:n*12], batch_size=16, shuffle=True)\n",
    "test_loader = tg.loader.DataLoader(dataset[n*12:], batch_size=16, shuffle=True)\n",
    "train_loader.dataset[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GINEConv(\n",
      "  (conv1): GINEConv(nn=Linear(in_features=1, out_features=10, bias=True))\n",
      "  (conv2): GINEConv(nn=Linear(in_features=10, out_features=20, bias=True))\n",
      "  (conv3): GINEConv(nn=Linear(in_features=20, out_features=10, bias=True))\n",
      "  (conv4): GINEConv(nn=Linear(in_features=10, out_features=1, bias=True))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_2060\\2296706662.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.processed_dir, 'data_{:05d}.pt'.format(idx)))\n"
     ]
    }
   ],
   "source": [
    "in_shape = dataset[0].num_features\n",
    "#out_shape = dataset[0].y.shape[1]\n",
    "#print(out_shape)\n",
    "class GINEConv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        self.conv1 = tg.nn.GINEConv(torch.nn.Linear(in_shape, 10), edge_dim=3)\n",
    "        self.conv2 = tg.nn.GINEConv(torch.nn.Linear(10, 20), edge_dim=3)\n",
    "        self.conv3 = tg.nn.GINEConv(torch.nn.Linear(20, 10), edge_dim=3)\n",
    "        self.conv4 = tg.nn.GINEConv(torch.nn.Linear(10, 1), edge_dim=3)\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = torch.tanh(self.conv1(x, edge_index, edge_attr))\n",
    "        x = torch.tanh(self.conv2(x, edge_index, edge_attr))\n",
    "        x = torch.tanh(self.conv3(x, edge_index, edge_attr))\n",
    "        x = torch.tanh(self.conv4(x, edge_index, edge_attr))  \n",
    "        out = x\n",
    "        return out\n",
    "model = GINEConv()\n",
    "#model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.NAdam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(loader):\n",
    "  model.train()\n",
    "  for data in loader:\n",
    "    #data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.edge_attr, data.batch)# Perform a single forward pass.\n",
    "    loss = criterion(out, data.y.argmax(dim=1))  # Compute the loss.\n",
    "    #print(loss)\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    optimizer.zero_grad() # Reset grads.\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        #data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        true = data.y.argmax(dim=1)\n",
    "        #print(true)\n",
    "        correct += int((true == pred).sum())\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_2060\\2296706662.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.processed_dir, 'data_{:05d}.pt'.format(idx)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m      5\u001b[0m     train(train_loader)\n\u001b[1;32m----> 6\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(test_acc)\n\u001b[0;32m      8\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(test_acc)\n",
      "Cell \u001b[1;32mIn[35], line 54\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# data = data.to(device)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m         pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Use the class with highest probability.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m         true \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert one-hot to class indices.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 16\u001b[0m, in \u001b[0;36mGINEConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index, edge_attr))\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x, edge_index, edge_attr))\n\u001b[1;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m)  \n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py:187\u001b[0m, in \u001b[0;36mGINEConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[0;32m    184\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gin_conv_GINEConv_propagate_7fq_fq67.py:157\u001b[0m, in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage_and_aggregate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gin_conv_GINEConv_propagate_7fq_fq67.py:92\u001b[0m, in \u001b[0;36mcollect\u001b[1;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_0, Tensor):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, \u001b[38;5;241m0\u001b[39m, _x_0)\n\u001b[1;32m---> 92\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:267\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_select_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\steph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:271\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_index_select_safe\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, index: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "acc = test(test_loader)\n",
    "accs.append(acc)\n",
    "for epoch in range(1, 30):\n",
    "    train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(test_acc)\n",
    "    accs.append(test_acc)\n",
    "    np.savetxt('accs.txt', accs)\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(accs, label='GINEConv')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
